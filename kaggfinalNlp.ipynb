{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b6c86-709a-4694-87e4-f63f892f9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AHMED AMIR RUSRUS\n",
    "# 221001678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9de664-00be-4f03-8819-dbec63cd18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1f043-d663-4791-a8f6-3f964bd252d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1Ô∏è‚É£ Extraction Step\n",
    "base_dir = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "datasets = {\n",
    "    \"redditsarcasm\": os.path.join(base_dir, \"redditsarcasm.zip\"),\n",
    "    \"goemotions\": os.path.join(base_dir, \"goemotions.zip\"),\n",
    "    \"dailydialogue\": os.path.join(base_dir, \"dailydialogue.zip\"),\n",
    "    \"empatheticdialogues\": os.path.join(base_dir, \"empatheticdialogues.tar.gz\")\n",
    "}\n",
    "\n",
    "extract_dirs = {}\n",
    "\n",
    "for name, archive_path in datasets.items():\n",
    "    extract_to = os.path.join(base_dir, name)\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    extract_dirs[name] = extract_to\n",
    "\n",
    "    if archive_path.endswith(\".zip\"):\n",
    "        with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"‚úÖ Extracted ZIP: {name}\")\n",
    "    elif archive_path.endswith(\".tar.gz\"):\n",
    "        with tarfile.open(archive_path, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall(extract_to)\n",
    "        print(f\"‚úÖ Extracted TAR.GZ: {name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Unknown archive format for {name}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Recursive Validation Function (Fixed)\n",
    "def validate_files_recursively(root_dir):\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            try:\n",
    "                if fname.endswith(\".csv\"):\n",
    "                    try:\n",
    "                        df = pd.read_csv(fpath, engine=\"python\", on_bad_lines='skip')\n",
    "                        print(f\"  ‚úÖ {fname}: {df.shape}, columns: {list(df.columns)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå CSV Error in {fname}: {e}\")\n",
    "                elif fname.endswith(\".tsv\"):\n",
    "                    try:\n",
    "                        df = pd.read_csv(fpath, sep=\"\\t\", engine=\"python\", on_bad_lines='skip')\n",
    "                        print(f\"  ‚úÖ {fname}: {df.shape}, columns: {list(df.columns)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå TSV Error in {fname}: {e}\")\n",
    "                elif fname.endswith(\".json\"):\n",
    "                    try:\n",
    "                        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                            data = json.load(f)\n",
    "                            summary = f\"{len(data)} items\" if isinstance(data, list) else f\"{len(data.keys())} keys\"\n",
    "                            print(f\"  ‚úÖ {fname}: JSON loaded ({summary})\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå JSON Error in {fname}: {e}\")\n",
    "            except Exception as outer:\n",
    "                print(f\"  ‚ùå Error reading {fname}: {outer}\")\n",
    "\n",
    "# 3Ô∏è‚É£ Validate All Datasets Recursively\n",
    "for name, path in extract_dirs.items():\n",
    "    print(f\"\\nüìÅ Validating dataset: {name}\")\n",
    "    validate_files_recursively(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7bc3c-feae-4f23-beba-b3cb2d5428b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDDIT SRCASM PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56f988-d10e-470c-9a99-6168fe4a76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Path to dataset\n",
    "sarcasm_dir = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\redditsarcasm\"\n",
    "\n",
    "# Output file\n",
    "output_file = \"processed/redditsarcasm.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Read and process function\n",
    "def process_sarcasm_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "        data = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            input_text = row.get(\"comment\", \"\")\n",
    "            label = row.get(\"label\", None)\n",
    "            if input_text and label in [0, 1]:\n",
    "                data.append({\n",
    "                    \"input\": str(input_text),\n",
    "                    \"response\": None,\n",
    "                    \"sarcasm\": bool(label),\n",
    "                    \"emotion\": None\n",
    "                })\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Merge all parts\n",
    "all_data = []\n",
    "for filename in [\"train-balanced-sarcasm.csv\", \"test-balanced.csv\", \"test-unbalanced.csv\"]:\n",
    "    file_path = os.path.join(sarcasm_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        all_data.extend(process_sarcasm_file(file_path))\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Save to JSONL\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Processed {len(all_data)} entries from RedditSarcasm into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bca11-eee8-4697-8daf-21c7b65c0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOEMOTIONS PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74596f09-01bf-49a8-923b-6edd5df43eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "goemotions_dir = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\goemotions\"\n",
    "output_file = \"processed/goemotions.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Load emotion mapping from label ID to emotion name\n",
    "mapping_file = os.path.join(goemotions_dir, \"emotion_mapping.json\")\n",
    "if os.path.exists(mapping_file):\n",
    "    with open(mapping_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        label_map = json.load(f)\n",
    "else:\n",
    "    # Fall back to default GoEmotions 28-label map if file not found\n",
    "    label_map = {\n",
    "        str(i): name for i, name in enumerate([\n",
    "            \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\",\n",
    "            \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\",\n",
    "            \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\",\n",
    "            \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\",\n",
    "            \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "        ])\n",
    "    }\n",
    "\n",
    "# Process function\n",
    "def process_tsv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"labels\", \"annotator\"], on_bad_lines='skip')\n",
    "        data = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[\"text\"]\n",
    "            raw_labels = str(row[\"labels\"]).split(\",\")\n",
    "            mapped = [label_map.get(lbl.strip(), None) for lbl in raw_labels if lbl.strip() in label_map]\n",
    "            if text and mapped:\n",
    "                data.append({\n",
    "                    \"input\": text,\n",
    "                    \"response\": None,\n",
    "                    \"sarcasm\": None,\n",
    "                    \"emotion\": mapped\n",
    "                })\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Merge all parts\n",
    "all_data = []\n",
    "for split in [\"train.tsv\", \"dev.tsv\", \"test.tsv\"]:\n",
    "    path = os.path.join(goemotions_dir, split)\n",
    "    if os.path.exists(path):\n",
    "        all_data.extend(process_tsv(path))\n",
    "    else:\n",
    "        print(f\"File not found: {path}\")\n",
    "\n",
    "# Save\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Processed {len(all_data)} entries from GoEmotions into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94851ff8-6cfe-4d95-b581-cceefd765750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empatheticdialogues preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b76fd9-a6cb-49ea-a784-bd87092e0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"üìÅ Processing: EmpatheticDialogues\")\n",
    "\n",
    "# Paths\n",
    "input_dir = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\empatheticdialogues\"\n",
    "output_file = \"processed/empatheticdialogues.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "all_data = []\n",
    "skipped_files = 0\n",
    "\n",
    "for filename in [\"train.csv\", \"valid.csv\", \"test.csv\"]:\n",
    "    path = os.path.join(input_dir, filename)\n",
    "    print(f\"üìÑ Reading: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(path, on_bad_lines='skip')\n",
    "        for _, row in df.iterrows():\n",
    "            utterance = str(row.get(\"utterance\", \"\")).strip()\n",
    "            emotion = str(row.get(\"context\", \"\")).strip()\n",
    "            if utterance and emotion:\n",
    "                all_data.append({\n",
    "                    \"input\": utterance,\n",
    "                    \"response\": None,\n",
    "                    \"sarcasm\": None,\n",
    "                    \"emotion\": emotion\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {filename}: {e}\")\n",
    "        skipped_files += 1\n",
    "\n",
    "# Save\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved {len(all_data)} entries to: {output_file}\")\n",
    "if skipped_files > 0:\n",
    "    print(f\"‚ö†Ô∏è Skipped {skipped_files} file(s) due to errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf24841-ac30-4006-8044-12f98a0d5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1f654-0823-45d5-ac44-86943962ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def inspect_csv_columns(dataset_dir):\n",
    "    for filename in [\"train.csv\", \"valid.csv\", \"test.csv\"]:\n",
    "        path = os.path.join(dataset_dir, filename)\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                df = pd.read_csv(path, on_bad_lines='skip', nrows=5)\n",
    "                print(f\"üìÑ {filename} ‚Üí Columns: {list(df.columns)}\")\n",
    "                print(df.head(1), \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Could not read {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {filename}\")\n",
    "\n",
    "inspect_csv_columns(r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\empatheticdialogues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f77bc-5016-47bf-b5dd-9414a672791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DailyDialog preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463a5b1-b3c3-4142-856c-fa55b37d82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "def process_dailydialogue(dataset_dir, output_file):\n",
    "    \"\"\"\n",
    "    Processes the DailyDialog dataset where each row contains a stringified list of utterances.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): Path to the dataset folder containing train.csv, etc.\n",
    "        output_file (str): Path to save the processed JSONL file.\n",
    "    \"\"\"\n",
    "    print(\"üìÅ Processing: DailyDialog\")\n",
    "\n",
    "    csv_path = os.path.join(dataset_dir, \"train.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"‚ùå File not found: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if 'dialog' not in df.columns:\n",
    "        print(\"‚ùå Column 'dialog' not found in CSV.\")\n",
    "        return\n",
    "\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        count = 0\n",
    "        skipped = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                utterances = ast.literal_eval(row['dialog'])\n",
    "                if not isinstance(utterances, list):\n",
    "                    raise ValueError(\"Parsed 'dialog' is not a list\")\n",
    "\n",
    "                for utt in utterances:\n",
    "                    if isinstance(utt, str) and utt.strip():\n",
    "                        out_f.write(json.dumps({\n",
    "                            \"text\": utt.strip(),\n",
    "                            \"source\": \"dailydialogue\"\n",
    "                        }, ensure_ascii=False) + \"\\n\")\n",
    "                        count += 1\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "\n",
    "    print(f\"‚úÖ Saved {count} utterances to: {output_file}\")\n",
    "    if skipped > 0:\n",
    "        print(f\"‚ö†Ô∏è Skipped {skipped} rows due to formatting issues.\")\n",
    "\n",
    "# ‚úÖ Run it\n",
    "process_dailydialogue(\n",
    "    dataset_dir=r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\dailydialogue\",\n",
    "    output_file=\"processed/dailydialogue.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5590a7-9306-4167-ba13-ea767157e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9e2f6-e0a7-4831-82ef-7cc93e8aac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dailydialog_path = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\dailydialogue\\train.csv\"  # Adjust if path is different\n",
    "df = pd.read_csv(dailydialog_path, on_bad_lines='skip')\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"First few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a64b2-14f9-44e4-8732-f504792ff919",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12daead4-a490-45f3-89fc-c46ec44880bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2 : THE REAL PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbcda0-7545-409b-8c5d-14d28a5a130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Directories\n",
    "input_dir = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\processed\"\n",
    "output_dir = \"processed_cleaned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Text Cleaning Function ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \" \", text)  # remove special characters but keep basic punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize whitespace\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# === Process All Datasets in JSONL ===\n",
    "def preprocess_all_cleaned():\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.endswith(\".jsonl\"):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        print(f\"\\nüìÅ Processing file: {filename}\")\n",
    "        cleaned_data = []\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for line in tqdm(infile, desc=\"  ‚Üí Cleaning entries\"):\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "\n",
    "                    # Apply cleaning\n",
    "                    entry[\"input\"] = clean_text(entry.get(\"input\", \"\"))\n",
    "                    if entry.get(\"response\"):\n",
    "                        entry[\"response\"] = clean_text(entry[\"response\"])\n",
    "\n",
    "                    cleaned_data.append(entry)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"‚ö†Ô∏è Skipped invalid JSON line.\")\n",
    "\n",
    "        # Save cleaned version\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for item in cleaned_data:\n",
    "                outfile.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Saved: {output_path} ({len(cleaned_data)} entries)\")\n",
    "\n",
    "# === Run Script ===\n",
    "preprocess_all_cleaned()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ae0ec-3b2d-4dbd-916b-d6ddda72e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64dc9e-a005-4216-96e6-f8516b666952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input and output paths\n",
    "input_dir = r\"C:\\Users\\RusRus\\Desktop\\datasetsnlp\\processed_cleaned\"\n",
    "output_path = \"final_dataset/merged_dataset.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Dataset-specific source labeling\n",
    "DATASETS = {\n",
    "    \"dailydialogue.jsonl\": \"dailydialogue\",\n",
    "    \"empatheticdialogues.jsonl\": \"empatheticdialogues\",\n",
    "    \"goemotions.jsonl\": \"goemotions\",\n",
    "    \"redditsarcasm.jsonl\": \"redditsarcasm\"\n",
    "}\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "for file_name, source in DATASETS.items():\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå File missing: {input_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÅ Merging from: {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # Limit Reddit Sarcasm to 250,000 samples\n",
    "        if source == \"redditsarcasm\":\n",
    "            lines = lines[:150000]\n",
    "\n",
    "        for line in tqdm(lines, desc=f\"Processing {source}\"):\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                merged_data.append({\n",
    "                    \"input\": entry.get(\"input\", \"\").strip(),\n",
    "                    \"response\": entry.get(\"response\", None),\n",
    "                    \"sarcasm\": entry.get(\"sarcasm\", None),\n",
    "                    \"emotion\": entry.get(\"emotion\", None),\n",
    "                    \"source\": source\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipped line due to error: {e}\")\n",
    "\n",
    "# Save merged output\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in merged_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Merged {len(merged_data)} entries to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5530b6d-adde-4da8-bc73-263e10c61d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‚úÖ Merged 304068 entries to: final_dataset/merged_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5f4e8-398c-4844-9f09-1053553ce301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # #  # # # # # # # # # # # #  # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b0801-284a-41dc-b77b-a4f6b29a8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE UNIFIED_MULTITASK MODEL :STEP1 ,Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f10e1-39f3-457c-b467-c4a166848e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ‚úÖ Updated paths for Kaggle\n",
    "input_dir = \"/kaggle/input/mnbjvghvu/datasetsnlp/final_merged\"  # Upload this folder with merged_dataset.jsonl\n",
    "input_file = os.path.join(input_dir, \"merged_dataset.jsonl\")\n",
    "output_dir = \"/kaggle/working/final_dataset_cleaned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"merged_dataset_cleaned.jsonl\")\n",
    "\n",
    "# üîß Text cleaning functions\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# üß† Load stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "stopwords = set(sklearn_stopwords)\n",
    "\n",
    "# üì¶ Processing loop\n",
    "valid_entries = []\n",
    "sarcasm_count = 0\n",
    "emotion_count = 0\n",
    "multitask_count = 0\n",
    "skip_count = 0\n",
    "\n",
    "print(f\"üìÅ Processing file: {input_file}\\n\")\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"‚Üí Cleaning entries\"):\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "\n",
    "            # Defensive coding\n",
    "            input_text = str(item.get(\"input\") or \"\").strip()\n",
    "            response = str(item.get(\"response\") or \"\").strip()\n",
    "            sarcasm = item.get(\"sarcasm\")\n",
    "            emotion = item.get(\"emotion\")\n",
    "\n",
    "            if not input_text:\n",
    "                skip_count += 1\n",
    "                continue\n",
    "\n",
    "            # Clean text\n",
    "            input_text = remove_stopwords(normalize_text(input_text), stopwords)\n",
    "            if response:\n",
    "                response = remove_stopwords(normalize_text(response), stopwords)\n",
    "\n",
    "            # Count sample type\n",
    "            if sarcasm is not None and emotion is not None:\n",
    "                multitask_count += 1\n",
    "            elif sarcasm is not None:\n",
    "                sarcasm_count += 1\n",
    "            elif emotion is not None:\n",
    "                emotion_count += 1\n",
    "\n",
    "            valid_entries.append({\n",
    "                \"input\": input_text,\n",
    "                \"response\": response if response else None,\n",
    "                \"sarcasm\": sarcasm,\n",
    "                \"emotion\": emotion\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            skip_count += 1\n",
    "\n",
    "# üíæ Save cleaned data\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_entries:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved cleaned dataset: {output_file} ({len(valid_entries)} entries)\")\n",
    "print(f\"üìä Stats:\")\n",
    "print(f\"   - Sarcasm-only samples: {sarcasm_count}\")\n",
    "print(f\"   - Emotion-only samples: {emotion_count}\")\n",
    "print(f\"   - Multitask (sarcasm + emotion): {multitask_count}\")\n",
    "print(f\"   - Skipped invalid lines: {skip_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e3fb1-9038-409c-93a0-80814aaaa702",
   "metadata": {},
   "outputs": [],
   "source": [
    "üìä Stats:\n",
    "   - Sarcasm-only samples: 149300\n",
    "   - Emotion-only samples: 142793\n",
    "   - Multitask (sarcasm + emotion): 0\n",
    "   - Skipped invalid lines: 11975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e738a-04a4-4b5c-aaf1-a2cbe0b8c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559cd998-116a-4bcd-882d-1bc7ac029445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture : loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de1a81-fa92-4f3a-9a15-91ef08e594eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 5 - Cell 1: Load libraries, set seed, load dataset\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# ‚úÖ Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ‚úÖ Load merged dataset\n",
    "merged_path = \"/kaggle/working/final_dataset_cleaned/merged_dataset_cleaned.jsonl\"\n",
    "with open(merged_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(raw_data):,} examples from {merged_path}\")\n",
    "\n",
    "# ‚úÖ Check class distribution\n",
    "sarcasm_count = sum(1 for x in raw_data if x[\"sarcasm\"] is not None)\n",
    "emotion_count = sum(1 for x in raw_data if x[\"emotion\"] is not None)\n",
    "multitask_count = sum(1 for x in raw_data if x[\"sarcasm\"] is not None and x[\"emotion\"] is not None)\n",
    "\n",
    "print(f\"üìä Distribution:\")\n",
    "print(f\"   - Sarcasm examples: {sarcasm_count:,}\")\n",
    "print(f\"   - Emotion examples: {emotion_count:,}\")\n",
    "print(f\"   - Multitask examples (both labels): {multitask_count:,}\")\n",
    "\n",
    "# ‚úÖ Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56ae08-8261-4703-b125-ecf51f341e50",
   "metadata": {},
   "outputs": [],
   "source": [
    " Loaded 292,093 examples from /kaggle/working/final_dataset_cleaned/merged_dataset_cleaned.jsonl\n",
    "üìä Distribution:\n",
    "   - Sarcasm examples: 149,300\n",
    "   - Emotion examples: 142,793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28a66b-21f7-48fa-b260-d8c297188965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply tokenization\n",
    "\n",
    "#Build dataset objects\n",
    "\n",
    "#Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddbc1a-f5ca-415a-beda-1b6af6481241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ‚úÖ Label mapping (you can update as needed)\n",
    "emotion_label2id = {\n",
    "    'admiration': 0, 'amusement': 1, 'anger': 2, 'annoyance': 3, 'approval': 4,\n",
    "    'caring': 5, 'confusion': 6, 'curiosity': 7, 'desire': 8, 'disappointment': 9,\n",
    "    'disapproval': 10, 'disgust': 11, 'embarrassment': 12, 'excitement': 13, 'fear': 14,\n",
    "    'gratitude': 15, 'grief': 16, 'joy': 17, 'love': 18, 'nervousness': 19,\n",
    "    'optimism': 20, 'pride': 21, 'realization': 22, 'relief': 23, 'remorse': 24,\n",
    "    'sadness': 25, 'surprise': 26, 'neutral': 27\n",
    "}\n",
    "\n",
    "# ‚úÖ Tokenize and prepare features\n",
    "def tokenize_entry(entry):\n",
    "    encodings = tokenizer(\n",
    "        entry[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoding = {key: val.squeeze(0) for key, val in encodings.items()}\n",
    "\n",
    "    # Handle sarcasm\n",
    "    encoding[\"sarcasm\"] = int(entry[\"sarcasm\"]) if entry.get(\"sarcasm\") is not None else -1\n",
    "\n",
    "    # Handle emotion (list, int, or str)\n",
    "    emotion = entry.get(\"emotion\")\n",
    "    if isinstance(emotion, list) and emotion:\n",
    "        label = emotion[0]\n",
    "    elif isinstance(emotion, str):\n",
    "        label = emotion\n",
    "    elif isinstance(emotion, int):\n",
    "        encoding[\"emotion\"] = emotion\n",
    "        return encoding\n",
    "    else:\n",
    "        encoding[\"emotion\"] = -1\n",
    "        return encoding\n",
    "\n",
    "    encoding[\"emotion\"] = emotion_label2id.get(label, -1)\n",
    "    return encoding\n",
    "\n",
    "# ‚úÖ Dataset class\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [tokenize_entry(x) for x in tqdm(data)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# ‚úÖ Train/Val split\n",
    "train_data, val_data = train_test_split(\n",
    "    raw_data,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ‚úÖ Build datasets and dataloaders\n",
    "train_dataset = MultiTaskDataset(train_data)\n",
    "val_dataset = MultiTaskDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "print(f\"‚úÖ DataLoaders ready: Train = {len(train_dataset)}, Val = {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0045d-8052-42cb-b961-6fad18154a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262883/262883 [01:04<00:00, 4059.14it/s]\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29210/29210 [00:07<00:00, 3763.12it/s]\n",
    "\n",
    "‚úÖ DataLoaders ready: Train = 262883, Val = 29210\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c36c29-44f5-480d-8a77-d7da36108de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A shared DistilBERT encoder\n",
    "\n",
    "#Two task-specific heads:\n",
    "\n",
    "    #One for sarcasm detection (binary classification)\n",
    "\n",
    "    #One for emotion classification (multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4236a2-c797-448e-9395-f81d8a0349a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_emotions: int):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        # ‚úÖ Sarcasm classification head\n",
    "        self.sarcasm_head = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 2)  # Binary classification\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Emotion classification head\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_emotions)  # Multi-class classification\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "\n",
    "        sarcasm_logits = self.sarcasm_head(pooled_output)\n",
    "        emotion_logits = self.emotion_head(pooled_output)\n",
    "\n",
    "        return sarcasm_logits, emotion_logits\n",
    "\n",
    "\n",
    "# ‚úÖ Define number of emotion classes\n",
    "num_emotions = len(emotion_label2id)\n",
    "\n",
    "# ‚úÖ Instantiate base model\n",
    "base_model = MultiTaskModel(num_emotions)\n",
    "\n",
    "# ‚úÖ Detect GPU(s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Available GPUs: {torch.cuda.device_count()} - Using: {device}\")\n",
    "\n",
    "# ‚úÖ Wrap in DataParallel if 2 GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"üöÄ Using DataParallel for multi-GPU training\")\n",
    "    model = nn.DataParallel(base_model)\n",
    "else:\n",
    "    model = base_model\n",
    "\n",
    "# ‚úÖ Send to device\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Model ready and sent to device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822c2a8-566c-4151-adbf-3e486ae29611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted CrossEntropyLoss for emotion classification (to handle imbalance)\n",
    "\n",
    "#CrossEntropyLoss for sarcasm (standard binary)\n",
    "\n",
    "#AdamW optimizer with weight decay\n",
    "\n",
    "#Linear learning rate scheduler with warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0f2b4-9936-4587-b170-55d4a655b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Compute emotion class weights to handle imbalance\n",
    "emotion_counts = Counter()\n",
    "for item in train_data:\n",
    "    if isinstance(item[\"emotion\"], list) and item[\"emotion\"]:\n",
    "        emotion_counts[item[\"emotion\"][0]] += 1\n",
    "    elif isinstance(item[\"emotion\"], str):\n",
    "        emotion_counts[item[\"emotion\"]] += 1\n",
    "\n",
    "emotion_weights = []\n",
    "total = sum(emotion_counts.values())\n",
    "for label in emotion_label2id:\n",
    "    count = emotion_counts.get(label, 1)  # avoid division by zero\n",
    "    emotion_weights.append(total / count)\n",
    "\n",
    "emotion_weights = torch.tensor(emotion_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# ‚úÖ Define loss functions\n",
    "sarcasm_criterion = nn.CrossEntropyLoss()\n",
    "emotion_criterion = nn.CrossEntropyLoss(weight=emotion_weights)\n",
    "\n",
    "# ‚úÖ Optimizer ‚Äî use `.parameters()` from model (wrapped or not)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# ‚úÖ Learning rate scheduler with warmup\n",
    "def lr_lambda(current_step):\n",
    "    warmup_steps = 500\n",
    "    total_steps = 10000  # adjust based on dataset size and batch size\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(0.0, float(1 - (current_step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(\"‚úÖ Loss functions, optimizer, and scheduler are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287eafe7-a704-443e-9f61-3f0ddddeed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop + Validation + Early Stopping\n",
    "\n",
    "#This includes:\n",
    "\n",
    "    #Epoch-based training with validation at each epoch\n",
    "\n",
    "    #Logging loss and accuracy for both sarcasm and emotion tasks\n",
    "\n",
    "    #Early stopping if validation doesn‚Äôt improve for N epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473a6b8-0d2d-4f01-a032-6ee38e1b0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA DEVICE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375e4e9-c320-4f74-ad06-b9ec11d01049",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"‚úÖ Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "model.device = device  # ‚¨ÖÔ∏è custom attribute used in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257c5e1-7282-4482-b749-4452d293cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# üß† Curriculum Helper\n",
    "def sort_dataset_by_input_length(dataset):\n",
    "    return sorted(dataset, key=lambda x: (x['input_ids'] != 0).sum(), reverse=False)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5, patience=2):\n",
    "    scaler = GradScaler('cuda')\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Get device from model properly\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_sarcasm_preds, train_sarcasm_labels = [], []\n",
    "        train_emotion_preds, train_emotion_labels = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"üß† Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sarcasm_labels = batch[\"sarcasm\"].to(device)\n",
    "            emotion_labels = batch[\"emotion\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                sarcasm_logits, emotion_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                valid_sarcasm_mask = sarcasm_labels != -1\n",
    "                valid_sarcasm_logits = sarcasm_logits[valid_sarcasm_mask]\n",
    "                valid_sarcasm_labels = sarcasm_labels[valid_sarcasm_mask]\n",
    "\n",
    "                valid_emotion_mask = emotion_labels != -1\n",
    "                valid_emotion_logits = emotion_logits[valid_emotion_mask]\n",
    "                valid_emotion_labels = emotion_labels[valid_emotion_mask]\n",
    "\n",
    "                loss_sarcasm = sarcasm_criterion(valid_sarcasm_logits, valid_sarcasm_labels)\n",
    "                loss_emotion = emotion_criterion(valid_emotion_logits, valid_emotion_labels)\n",
    "                loss = loss_sarcasm + loss_emotion\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            train_sarcasm_preds += torch.argmax(valid_sarcasm_logits, dim=1).cpu().tolist()\n",
    "            train_sarcasm_labels += valid_sarcasm_labels.cpu().tolist()\n",
    "            train_emotion_preds += torch.argmax(valid_emotion_logits, dim=1).cpu().tolist()\n",
    "            train_emotion_labels += valid_emotion_labels.cpu().tolist()\n",
    "\n",
    "        train_sarcasm_acc = accuracy_score(train_sarcasm_labels, train_sarcasm_preds)\n",
    "        train_emotion_acc = accuracy_score(train_emotion_labels, train_emotion_preds)\n",
    "\n",
    "        # üìä Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_sarcasm_preds, val_sarcasm_labels = [], []\n",
    "        val_emotion_preds, val_emotion_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"üîç Validating Epoch {epoch+1}\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                sarcasm_labels = batch[\"sarcasm\"].to(device)\n",
    "                emotion_labels = batch[\"emotion\"].to(device)\n",
    "\n",
    "                with autocast('cuda'):\n",
    "                    sarcasm_logits, emotion_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                    valid_sarcasm_mask = sarcasm_labels != -1\n",
    "                    valid_sarcasm_logits = sarcasm_logits[valid_sarcasm_mask]\n",
    "                    valid_sarcasm_labels = sarcasm_labels[valid_sarcasm_mask]\n",
    "\n",
    "                    valid_emotion_mask = emotion_labels != -1\n",
    "                    valid_emotion_logits = emotion_logits[valid_emotion_mask]\n",
    "                    valid_emotion_labels = emotion_labels[valid_emotion_mask]\n",
    "\n",
    "                    loss_sarcasm = sarcasm_criterion(valid_sarcasm_logits, valid_sarcasm_labels)\n",
    "                    loss_emotion = emotion_criterion(valid_emotion_logits, valid_emotion_labels)\n",
    "                    loss = loss_sarcasm + loss_emotion\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                val_sarcasm_preds += torch.argmax(valid_sarcasm_logits, dim=1).cpu().tolist()\n",
    "                val_sarcasm_labels += valid_sarcasm_labels.cpu().tolist()\n",
    "                val_emotion_preds += torch.argmax(valid_emotion_logits, dim=1).cpu().tolist()\n",
    "                val_emotion_labels += valid_emotion_labels.cpu().tolist()\n",
    "\n",
    "        val_sarcasm_acc = accuracy_score(val_sarcasm_labels, val_sarcasm_preds)\n",
    "        val_emotion_acc = accuracy_score(val_emotion_labels, val_emotion_labels)\n",
    "\n",
    "        print(f\"\\nüìâ Epoch {epoch+1} Summary:\")\n",
    "        print(f\"   Train Loss: {total_train_loss:.4f} | Sarcasm Acc: {train_sarcasm_acc:.4f} | Emotion Acc: {train_emotion_acc:.4f}\")\n",
    "        print(f\"   Val Loss:   {total_val_loss:.4f} | Sarcasm Acc: {val_sarcasm_acc:.4f} | Emotion Acc: {val_emotion_acc:.4f}\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# ‚úÖ SAFE ENTRY POINT\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: suppress HuggingFace tokenizers fork warning\n",
    "    import os\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    sorted_train_data = sort_dataset_by_input_length(train_dataset)\n",
    "    train_loader = DataLoader(\n",
    "        sorted_train_data,\n",
    "        batch_size=8,\n",
    "        shuffle=False,  # ‚ö†Ô∏è Sorted, so don't shuffle\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    trained_model = train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd7d4f-b673-43d3-a0af-c5b1111dd7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03000e-2039-4428-a4c8-61c767a93a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb4866-7ba9-4851-83f4-9c6e12e5c04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
